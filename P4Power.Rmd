---
title: "P for POWER"
subtitle: Statistical Inference
author: "Linda Angulo Lopez"
job: Sustainability Hacker
date: "02/01/2021"
output:
  powerpoint_presentation: default
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
fig_width: 7
fig_height: 3
widgets: mathjax
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## R4.0 Environment


```{r,  R_environment, results='hide',  warning=FALSE , message=FALSE}
#R4.0 Environmental Set for:
library(knitr) # creating a pdf document ; 
library(ggplot2) # making plots
library(reshape2) # data frames

```






_library(knitr)_ # creating slides

_library(ggplot2)_ # making plots

_library(reshape2)_ # handling data frames


# CASE: [Respiratory Distress Index and Sleep Disturbances](https://www.coursera.org/specializations/jhu-data-science#courses)




:: $H_0$    is     $\mu_0$ = 30 

:: $H_a$    is     $\mu_a$ > 30


## [Central Theory Limit](https://lindangulopez.github.io/statistical-inference/)

Where (X'-30)/($s$/$\sqrt(n)$) measures the number of standard 

errors the sample mean is from the mean hypothesized by $H_0$

and the denominator, ($s$/$\sqrt(n)$) is the standard error of 

the sample mean.



## PS: if $H_a$ specified that $\mu_a$ < $\mu_0$



:: flip the following reasoning

:: look at the right tail



## POWER



Power is the probability of rejecting the NULL HYPOTHESIS

$H_0$, when it is false. 



:: Used to determine if your sample size was big enough

to yield a meaningful, rather than random result




:: Detect if your ALTERNATIVE HYPOTHESIS, $H_a$ is true, 

to lower the risk of a Type II errors.


## Equation





As beta, $\beta$, is the probability of a _Type II error,_

for accepting a false null hypothesis, then the complement 

of this the power is (1-$\beta$).


\[
P = 
    (1-\beta)
\]


## Do you remember this ... ?




:: As $\mu_a$ gets bigger, the test gets more powerful

:: As $n$ gets bigger, the test gets more powerful

:: Power decreases with increases in variation

:: As $\alpha$ increases, power increases

:: In a one sided test the power is greater as $\alpha$ 

is bigger than $\alpha$/2 



## Alpha



Power is the probability that the true mean $\mu$ is greater 

than the (1-$\alpha$) quantile, in our sleep example:

:: if $\alpha$ = .05 and as $H_a$  $\mu_a$ > 30


:: for normal distributions of which we know the variances


:: we make p = qnorm(.95) our reference


## When to reject H_0 ?


```{r conf_5pct, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

x <- seq(-4,4, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)+scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(x>qnorm(.95),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE))) 
#suppressWarnings(g <- g + geom_line(x=2.0,size=1.5,colour="blue"))
suppressWarnings(print(g))
```

If a test statistic fell in the shaded portion, 5% of the 
area under the curve, we would reject $H_0$ in favor $H_a$.

## Two distributions




The two hypotheses, $H_0$ and $H_a$, represent two distributions

:: since they're talking about means or 

:: centers of distributions. 


## [Normal distributed](https://math.stackexchange.com/questions/217176/normal-distribution-the-y-value#:~:text=the%20familiar%20%22bell%2Dshaped%22,1%E2%88%9A2%CF%80%CF%83.) 




For a _random variable_ X which is distributed as _Normal_ 

with a mean _mu_, $\mu$ and variance _sigma squared_, $\sigma^2$:


:: under $H_0$,      X' is       \sim N($\mu_0$ , $\sigma^2$/n)


:: under $H_a$,      X' is       \sim N($\mu_a$ , $\sigma^2$/n)


## 95th percentile of $H_0$, black vertical on the red plot

```{r twoDistros_Ho, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 32
  mu0 <- 30
  sigma <- 4
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```



## Mean proposed by $H_a$, peak of blue plot

```{r twoDistros_Ha, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 32
  mu0 <- 30
  sigma <- 4
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```




## Power is how far $H_a$ is from the right of 95th percentile of $H_0$

```{r twoDistros_HaHo, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 32
  mu0 <- 30
  sigma <- 4
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```

## Power depends on the null distribution's variance

```{r twoDistros_fat, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 32
  mu0 <- 30
  sigma_fat <-8
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma_fat / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```



## If $\mu_a$ = 34 > $\mu_0$ the test, $H_a$ is more powerful than $H_0$

```{r twoDistros_ma_34, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 34
  mu0 <- 30
  sigma_fat <-8
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma_fat / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```


## If power is large, we can reject the null hypothesis

The distribution represented by $H_a$ will moved to the right, 

most of the blue curve is to the right of the vertical line, 

indicating that with $\mu_a$ = 34, the test is more powerful, 

so it is 

:: correct to reject the $H_0$ as it appears to be false.



## If $\mu_a$ = 30 = $\mu_0$ the power is at $\alpha$

```{r twoDistros_ma_30, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 30
  mu0 <- 30
  sigma_fat <-8
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma_fat / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```



## If power is similar or equal to the 95th percentile of $H_0$, we cannot reject the null hypothesis

The distribution represented by $H_a$, in the above graph 

moved under the blue curve, indicating that with 

:: $\mu_a$ = 30 = $\mu_0$ , the test, 

:: $H_a$ is almost as powerful as $H_0$, so it is 

:: incorrect to reject the null hypothesis since it does 

not appear to be false. 


## If $\mu_a$ = 28 < $\mu_0$ the test's power is weaker


```{r twoDistros_ma_28, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

  mua <- 28
  mu0 <- 30
  sigma_fat <-8
  n <- 16
  alpha <- .05
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mu0, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "red")
  g = g + stat_function(fun=dnorm, geom = "line", 
                        args = list(mean = mua, sd = sigma_fat / sqrt(n)), 
                        size = 2, col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma_fat / sqrt(n)
  g = g + geom_vline(xintercept=xitc, size = 3)
  print(g)
  
```

## Not worth investigating




The distribution represented by $H_a$ will move to the left of 

$\mu_0$ = 30, the area under the blue curve is less than the 

5% our $\alpha$, so the test is not only less powerful, it even 

contradicts $H_a$

:: it is therefore not worth looking into.



## RECAP




Power is a function that depends on a specific value of an 

alternative mean, $\mu_a$, which is any value greater than 

$\mu_0$, the mean hypothesized by $H_0$


:: Recall that H_a specified mu > 30, in the sleep case.




## RECAP




If $\mu_a$ is much bigger than $\mu_0$ = 30 then the power 

a probability, is bigger than if $\mu_a$ is close to 30. 


:: As $\mu_a$ approaches 30, the mean under $H_0$, the power 

approaches $\alpha$.



## Power Curves $n$

```{r PowerCurves_1, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

z <- qnorm(.95)

nseq = c(8, 16, 32, 64, 128)
mua = seq(30, 35, by = 0.1)
power = sapply(nseq, function(n)
  pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
        lower.tail = FALSE)
)
colnames(power) <- paste("n", nseq, sep = "")
d <- data.frame(mua, power)
d2 <- melt(d, id.vars = "mua")
names(d2) <- c("mua", "n", "power")    
g <- ggplot(d2, 
            aes(x = mua, y = power, col = n)) + geom_line(size = 2)
print(g)
  
```


##  z <- qnorm(.95) 

```{r define_z, echo = TRUE}

# mean=30

z <- qnorm(.95)

pnorm(30+z,mean=30,lower.tail=FALSE) 

```

With the mean set to $\mu_0$ the two distributions, 

null and alternative, are the same and power = $\alpha$ = 5%.


##  $\mu_a$ > $\mu_0$

```{r pnorm, echo = TRUE}

#mean=32

z <- qnorm(.95)

pnorm(30+z, mean=32,lower.tail=FALSE) 

```

With $\mu_a$ > $\mu_0$mu_a the power is greater than $\alpha$, at 64%.

:: When the sample mean is many standard errors greater than 

the mean hypothesized by the null hypothesis, 

:: the probability of rejecting $H_0$ is false is much higher. 

##  Standard deviation, sd

```{r SE_1, echo = TRUE}
# sd=1

z <- qnorm(.95)

pnorm(30+z, mean=32, sd= 1, lower.tail=FALSE) 

# sd=2

z <- qnorm(.95)

pnorm(30+z, mean=32, sd= 2, lower.tail=FALSE) 

```

:: Power decreases with increases in variation


## Power Curves, $\sigma$

```{r PowerCurves_2, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

z <- qnorm(.95)
mu0 <- 30
n <- 16
sigseq = c(8, 16, 32, 64, 128)
mua = seq(30, 35, by = 0.1)
power = sapply(sigseq, function(sigma)
  pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
        lower.tail = FALSE)
)
colnames(power) <- paste("sigma", sigseq, sep = "")
d <- data.frame(mua, power)
d2 <- melt(d, id.vars = "mua")
names(d2) <- c("mua", "sigma", "power")    
g <- ggplot(d2, 
            aes(x = mua, y = power, col = sigma)) + geom_line(size = 2)
print(g)   
  
```

## RECAP

:: As $\alpha$ increases, power increases

:: In a one sided test the power is greater as 
$\alpha$ is bigger than $\alpha$/2

## Power Curves, $\alpha$

```{r PowerCurves_3, fig.width = 7, fig.height =3, fig.asp = .618, warning=FALSE , message=FALSE, echo=FALSE}

alseq = c(.95, .9, .85, .8)
mua = seq(30, 35, by = 0.1)
z = lapply(alseq, qnorm)
power = sapply(z, function(z)
   pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
        lower.tail = FALSE)
)
colnames(power) <- paste("alpha", 1-alseq, sep = "")
d <- data.frame(mua, power)
d2 <- melt(d, id.vars = "mua")
names(d2) <- c("mua", "alpha", "power")    
g <- ggplot(d2, 
            aes(x = mua, y = power, col = alpha)) + geom_line(size = 2)
print(g)  
  
```


# if H_a specified that $\mu_a$ < $\mu_0$

:: flip the above reasoning

:: look at the right tail

## Back to the sleep case

Assumed that X'~ N($\mu_a$, $\sigma^2/n$)

Supposed that $H_a$ says that $\mu$ > $\mu_0$

:: As power = 1 - beta 

= Prob ( X' > $\mu_0$ +  z_(1-$\alpha$) * $\sigma$/$\sqrt(n)$) 

 



## When $H_a$ says that $\mu$ > $\mu_0$ we know 

:: the population mean equals $\mu_0$

:: the level size of the test is $\alpha$




## We do not know $\beta$, $\sigma$, $n$, and $\mu_a$

:: But are given $\mu_a$ > $\mu_0$, $\mu_0$ and $\alpha$ 

:: and X' depends on the data. 


#### So we can solve for the quantities $\beta$, $\sigma$, $n$,and $\mu_a$


:: As power = 1 - beta 

= Prob ( X' > $\mu_0$ +  z_(1-$\alpha$) * $\sigma$/$\sqrt(n)$) 


## Solve for $P$, power or $n$, the sample size

:: only $\sqrt(n)$ * $\frac{\mu_a - \mu_0}{\sigma}$ is needed

:: this is the effect size


## Example: t distributions


:: t quantile instead of the z


Power is still a probability, namely 

:: $\frac{P(X' - \mu_0)}{(S /\sqrt(n)}$ > t_(1-$\alpha$, $n$-1) 

for $H_a$ that $\mu$ > $mu_a$  


:: the proposed distribution is not centered at $mu_0$,

:: so we have to use the non-central t distribution.

## R function power.t.test()

:: omit one of the arguments and power.t.test()

solves for it

```{r Rfun_power_t_test,  echo=TRUE }

power.t.test(n = 16,delta = 2 / 4, sd=1, type = "one.sample", 
             alt = "one.sided")$power

power.t.test(n = 16, delta = 2 , sd=4, type = "one.sample", 
             alt ="one.sided")$power

power.t.test(n = 16, delta = 100 , sd=200, type = "one.sample", 
             alt ="one.sided")$power

```

## Constant Power

:: keeping the effect size (the ratio delta/sd) constant preserved the power

:: get a constant effect size with 

_with the same values for n (16) & alpha (.05), but _

_different delta and standard deviation values_


## Solve for $n$

:: specify a power we want and solve for the sample size $n$.


```{r Rfun_power_t_test2, echo = TRUE}

power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample", 
             alt = "one.sided")$n

power.t.test(power = .8, delta = 2, sd=4, type = "one.sample", 
             alt = "one.sided")$n

power.t.test(power = .8, delta = 100, sd=200, type = "one.sample", 
             alt = "one.sided")$n
```

## Solve for $\delta$ change $n$

```{r Rfun_power_t_test3, echo = TRUE}
# use power.t.test to find delta for a power=.8 and n=26 and sd=1

power.t.test(power = .8, n=26, sd=1, type = "one.sample",  
             alt = "one.sided")$delta

#use power.t.test to find delta for a power=.8 and n=27 and sd=1

power.t.test(power = .8, n=27, sd=1, type = "one.sample",  
             alt = "one.sided")$delta
```


## Solve for $\delta$ change $sd$

```{r Rfun_power_t_test4, echo = TRUE}
#use power.t.test to find delta for a power=.8 and n=26 and sd=2

power.t.test(power = .8, n=26, sd=2, type = "one.sample",  
             alt = "one.sided")$delta
```


# [POWERful RECAP](https://www.coursera.org/learn/statistical-inference) 

:: As $\mu_a$ gets bigger, the test gets more powerful

:: As $n$ gets bigger, the test gets more powerful

:: Power decreases with increases in variation

:: As $\alpha$ increases, power increases, so 

:: A one sided test has greater power 

# @lindangulopez


### [myCOURSERA notes:](https://www.coursera.org/specializations/jhu-data-science) 


### [plots from swirl](https://swirlstats.com/)
